# NCCL useful insight

1.æåˆ°NCCLçš„topology-awareç‰¹æ€§ä»¥åŠè¯¥ç‰¹æ€§çš„ä½œç”¨

One of the primary techniques employed today to minimize the impact of communication is to overlap it behind compute. This is enabled via clever fabric topologies [17], [26] accompanied by **topology-aware collective communication** algorithms (e.g., Ring and Double-binary tree for All-reduce) implementations in libraries like Intelâ€™s MLSL [23]/oneCCL [7] and NVIDIAâ€™s NCCL [42].

> æ¥è‡ªï¼šEnabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms(2021, ISCA)

2.æåˆ°NCCLçš„GPUDirectæŠ€æœ¯å¦‚ä½•é€ æˆäº†è®¡ç®—ä¸é€šä¿¡ä¹‹é—´çš„äº’ç›¸å½±å“ï¼Œè¿›è€Œå¸¦æ¥slowdown.

> å°±æ˜¯è¯´ï¼ŒNCCLå¯ä»¥åˆ©ç”¨GPUDirectæ¥å®ç°GPUçº¿ç¨‹ä¹‹é—´çš„ç›´æ¥é€šä¿¡ï¼Œä»è€Œå‡å°‘CPUçš„å‚ä¸ï¼Œå¹¶æé«˜é€šä¿¡æ•ˆç‡ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™ç§æœºåˆ¶å…è®¸GPUçº¿ç¨‹è¿›è¡Œæ•°æ®ä¼ è¾“ï¼Œä½†æ˜¯ç”±äºç°æœ‰ç¡¬ä»¶çš„é™åˆ¶ï¼ˆgapï¼‰ï¼Œåº•å±‚è¦ä½¿ç”¨MMIOæ¥å®ç°ã€‚è€ŒMMIOæœ€ç»ˆä¼šå¸¦æ¥é—®é¢˜

Since CPU intervention incurs a large overhead, how about managing the communication with GPU itself? NCCL leverages GPUDirect to enable this approach, which exposes the GPU memory space for peer-to-peer access so that GPU threads can read/write data to/from another GPU.

As GPU threads can directly invoke data copy, they can handle communication events efficiently without the involvement of CPU. Since commodity GPU hardware disallows GPU threads to initiate its own DMA engine, GPU-controlled communication leverages MMIO, which will implicitly conduct DMA when GPU threads write data on the mapping. Figure 3 compares CPU-controlled and GPU-controlled communication. The former one (Figure 3a) takes the following steps: 

 1.CPU is notified when the data is ready

 2.CPU initiates the DMA engine

 3.DMA copies the data. 

On the other hand, GPU-controlled communication with MMIO (Figure 3b) follows 

1.CPU creates a memory map (mmap) of the destination GPUâ€™s address space prior to runtime execution

2.the data is ready at runtime

3.GPU threads copy the data into the mmap, which implicitly conducts DMA copy. 

**Unfortunately, data copying by GPU threads often heavily interferes with parallel kernel computation, especially due to L2 cache pollution and warp scheduler operations**.

![](../../notes/figures/19.png)

> æ¥è‡ªARK: GPU-driven Code Execution for Distributed Deep Learning(2023, NSDI)
>
> NVIDIA. GPUDirect. https://developer.nvidia.com/gpudirect
>
> [GPUDirect RDMA](https://docs.nvidia.com/cuda/gpudirect-rdma/) (Developing a Linux Kernel Module using GPUDirect RDMAğŸ˜€)
>
> https://github.com/FelixFu520/README/blob/main/envs/gpus/gpudirect.md (â­GPUé€šä¿¡è¯¦è§£ï¼)